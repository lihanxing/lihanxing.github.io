---
layout: post
title:  "Nerf学习"
date:   2022-07-7
categories: 三维重建
---
* content
{:toc}

---
三维重建表示方法分类：显示表示，隐式表示
* 显示表示方法：

	利用mesh、点云、volume等对物体或场景进行重建。优点是**能够合成出照片级的虚拟视角**。缺点是**不够精细、且对内存的消耗过大。限制了此类方法对大范围高分辨率场景的表示。**

* 隐式表示方法

	用一个函数来描述几何场景。比如在深度学习方法中，神经网络只是模拟出三维物体在空间中的占用，后续再通过渲染显示的获得其三维形状。优点是**隐式的表示是一种连续的表示，能够适用于分辨率高的大范围场景。**



## 相关知识
---
### 神经场
场是为所有(连续)空间/或时间坐标定义的量(标量)，如电磁场、重力场之类的，我们在讨论场的时候，其实是在讨论一个连续的概念，它将一个高维的向量映射到标量。

神经场表示通过神经网络进行全部or局部参数化的场。

在视觉领域，我们可以认为**神经场就是以空间坐标or其它维度信息(时间、相机位姿)作为输入，通过一个MLP网络模拟目标函数，生成一个目标标量(颜色、深度等)的过程。**

----
### 体渲染
关于体渲染的理解可以是，从三维场景渲染2D图像的过程。

---
Nerf最大的贡献就是，将神经场和体渲染结合起来，

---
## Nerf

Nerf简单的说就是，用多张带位姿的2D图像作为监督，去重建三维场景。

Nerf的主要任务：在三维重建的过程中，根据位姿和图像，合成新视角下的图像。

Nerf通过MLP结构，将3D sense表达为一个可学习且连续的辐射场*F<sub>$\theta$</sub>(**x**,**d**)=[$\sigma$,**c**]*,**x** = (x,y,z)代表了空间中某点的坐标，**d**=(d<sub>$x$</sub>,d<sub>$y$</sub>,d<sub>$z$</sub>)为观测方向，$\sigma$代表改点的体密度估计值，换句话说是光线在改点的终止概率。**c**=(r,g,b)为该点在改视角下的颜色估计值，$\theta$为网络参数。

```
我的理解：Nerf有些像游戏中渲染那样，当视角变化时，我们看到的效果也会发生变化，Nerf就是模拟这个过程。
通过多张图像，去学习到新视角看到这个场景的效果。
因为通过多张图像已经包含了同一场景/物体在不同视角下的效果，神经网络就去学习这个现象，然后通过已有的现象，去猜/推断一个前所未有的视角。
```
### 基本思想

输入：3D 位置 *x=(x,y,z)*,2D视角方向d
输出：对应的颜色**c**=(r,g,b)，体积密度$\sigma$。
在实际的实现中，视角方向表示为一个三维笛卡尔坐标系单位向量**d**，即图像中任意位置与相机光心的连线。用一个 MLP 全连接网络表示这种映射：*F<sub>$\theta$</sub>(**x**,**d**)=[$\sigma$,**c**]*,通过优化网络参数$\theta$，来实现输入5D坐标，输出对应的颜色和密度的映射。
![2D视角方向示意图](/img\Nerf\2D视角方向示意图.png)


为了让网络学习到多视角的表示，做一下两个合理的假设：
* 体积密度/不透明度$\sigma$只与三维位置**x**有关，而与视角方向**d**无关。物体不同位置密度与观察角度无关，因为一个点处的不透明度，它自身就已经确定了，人类改变观察它的视角，也是不能改变它的不透明度。
* 空间中某点的颜色，是受位置与观察角度的影响。
  
所以预测体积密度$\sigma$的网络部分输入仅仅是位置**x**，预测颜色**c**的网络的输入是视角方向**d**。具体实现为：

* MLP网络F<sub>$\theta$</sub>首先用 8 层的全连接层（使用 ReLU 激活函数，每层有 256 个通道），处理 3D 坐标**x**，得到体积密度$\sigma$和一个256维的特征向量。
* 将该256维的特征向量与视角方向**d**concact起来，喂给另一个全连接层（使用 ReLU 激活函数，每层有 128 个通道），输出方向相关的 RGB 颜色。

所谓的5D神经辐射场将场景表示为：**其所在空间中任意点的体素密度和有方向的辐射亮度**。体积密度$\sigma$(x)定义为光线停留在位置x处无穷小粒子的可导概率（或者也可以理解为光线穿过此点后终止的概率）。
那么对于某个视角O发出的方向为d的光线，其在t时刻的到达点为：`r(t)=o+td`。
沿着这个方向在范围(t<sub>n</sub>,t<sub>f</sub>)对颜色积分，获得最终的颜色值*C*(r)为：
![](/img\Nerf\C(r).png)
其中,*T(t)*表示从t<sub>n</sub>到t累计的透明度，换句话说就是光线从t<sub>n</sub>到t没有碰到任何粒子的概率。因此视图的渲染就是对于*C*(r)的积分，是就是虚拟相机穿过每个像素的相机光线，所得到的颜色。不过在实际中，连续的积分是不现实的，因此使用求积法进行积分的数值求解

### 优化
* Positional Encoding (位置编码)：通过这一策略，能够使得 MLP 更好地表示高频信息，从而得到丰富的细节；
* Hierarchical Sampling Procedure (金字塔采样方案)：通过这一策略，能够使得训练过程更高效地采样高频信息。
#### Positional Encoding (位置编码)
将F<sub>$\theta$</sub>修改成两个函数的组合：F<sub>$\theta$</sub>=F<sub>$\theta$</sub><sup>'</sup>o$\gamma$,通过这样方式提升网络的性能。
$\gamma$表示从低维到高维的编码函数。
F<sub>$\theta$</sub><sup>'</sup>是正常的MLP函数。

#### Hierarchical Sampling Procedure (金字塔采样方案)
如果采样点过多计算效率太低，采样点太少又不能很好地近似整个三维场景。一个自然的想法就是对于颜色贡献大的点附件进行密集采样，贡献少的就稀疏采样，基于这一想法，NeRF 很自然地提出由粗到细的分层采样方案（Coarse to Fine）。


---

## object-Nerf
目的：
传统的Nerf渲染场景的时候，将整个场景作为整体，统一的渲染，这样的结果缺少了可编辑性，所以有较大的提升空间。在object-Nerf中解决了这个问题，不仅仅对整个场景进行渲染，也可以对场景中的物体进行编辑(旋转、平移、复制等)，实现可编辑的Nerf。

![](/img\Nerf\Object-Nerf示意图.png)
<center>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">object-Nerf示意图</div></center>

![](/img\Nerf\Object-nerf_网络结构.png)
<center>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">object-Nerf网络结构</div></center>

上图为object-Nerf的网络结构。对于object-Nerf而言，它的输入是空间中某一点x它的混合空间嵌入特征向量，
混合空间嵌入特征向量由体素特征与位置坐标特征组成，体素特征指的是将三维空间体素化，对于x所待的体素块，周围由其它8个体素块围着它，那就由其它8个体素块组成的特征，描述x所在的体素块。
从上图可以看到，object-Nerf网络由scene branch和object-branch两个分支组成。
### scene branch
scene branch就和传统的Nerf结构一样，学习整个三维空间的表达。它学到的东西，有两个作用：1.作为后续object-Nerf的背景 2.为后面的object-branch中，判断物体是否被遮挡。

### object branch
object branch的功能就是，对每个单个物体进行渲染。在object branch中，它是以mask作为约束，只学习mask中物体的颜色属性、不透明度等等，对每个单独物体进行Nerf化。同时，场景中一般有多个物体，那么获得相应的n个物体的激活码。在渲染过程中，我们对于在mask中的每一条射线，它对应一个物体，我们为该射线赋予对应的激活码，让激活码和我们物体关联起来，这样再后面的渲染时，输入对应的激活码，生成对应的物体。

### scene-guided occlusion identification
在现实中，物体之间有遮挡是很正常的情况，那在网络中，如果我们将两个物体的遮挡交界处学到，那势必会对这个物体的Nerf化有影响。作者人为的将相机视角，往前推了一部分，这样就避开了物体的遮挡交界处，虽然学到的部分表少了，但是物体不会收到遮挡交界处的影响，再通过其他视角对于遮挡部分进行补充。
总之：只学习未被遮挡的部分，不学习遮挡区域。