---
layout: post
title:  "Nerf学习"
date:   2022-07-7
categories: 三维重建
---
* content
{:toc}

---
三维重建表示方法分类：显示表示，隐式表示
* 显示表示方法：

	利用mesh、点云、volume等对物体或场景进行重建。优点是**能够合成出照片级的虚拟视角**。缺点是**不够精细、且对内存的消耗过大。限制了此类方法对大范围高分辨率场景的表示。**

* 隐式表示方法

	用一个函数来描述几何场景。比如在深度学习方法中，神经网络只是模拟出三维物体在空间中的占用，后续再通过渲染显示的获得其三维形状。优点是**隐式的表示是一种连续的表示，能够适用于分辨率高的大范围场景。**



## 相关知识
---
### 神经场
场是为所有(连续)空间/或时间坐标定义的量(标量)，如电磁场、重力场之类的，我们在讨论场的时候，其实是在讨论一个连续的概念，它将一个高维的向量映射到标量。

神经场表示通过神经网络进行全部or局部参数化的场。

在视觉领域，我们可以认为**神经场就是以空间坐标or其它维度信息(时间、相机位姿)作为输入，通过一个MLP网络模拟目标函数，生成一个目标标量(颜色、深度等)的过程。**

----
### 体渲染
关于体渲染的理解可以是，从三维场景渲染2D图像的过程。

---
Nerf最大的贡献就是，将神经场和体渲染结合起来，

---
## Nerf

Nerf简单的说就是，用多张带位姿的2D图像作为监督，去重建三维场景。

Nerf的主要任务：在三维重建的过程中，根据位姿和图像，合成新视角下的图像。

Nerf通过MLP结构，将3D sense表达为一个可学习且连续的辐射场*F<sub>$\theta$</sub>(**x**,**d**)=[$\sigma$,**c**]*,**x** = (x,y,z)代表了空间中某点的坐标，**d**=(d<sub>$x$</sub>,d<sub>$y$</sub>,d<sub>$z$</sub>)为观测方向，$\sigma$代表改点的体密度估计值，换句话说是光线在改点的终止概率。**c**=(r,g,b)为该点在改视角下的颜色估计值，$\theta$为网络参数。

```
我的理解：Nerf有些像游戏中渲染那样，当视角变化时，我们看到的效果也会发生变化，Nerf就是模拟这个过程。
通过多张图像，去学习到新视角看到这个场景的效果。
因为通过多张图像已经包含了同一场景/物体在不同视角下的效果，神经网络就去学习这个现象，然后通过已有的现象，去猜/推断一个前所未有的视角。
```
### 基本思想

输入：3D 位置 *x=(x,y,z)*,2D视角方向d
输出：对应的颜色**c**=(r,g,b)，体积密度$\sigma$。
在实际的实现中，视角方向表示为一个三维笛卡尔坐标系单位向量**d**，即图像中任意位置与相机光心的连线。用一个 MLP 全连接网络表示这种映射：*F<sub>$\theta$</sub>(**x**,**d**)=[$\sigma$,**c**]*,通过优化网络参数$\theta$，来实现输入5D坐标，输出对应的颜色和密度的映射。
![2D视角方向示意图](/img\Nerf\2D视角方向示意图.png)


为了让网络学习到多视角的表示，做一下两个合理的假设：
* 体积密度/不透明度$\sigma$只与三维位置**x**有关，而与视角方向**d**无关。物体不同位置密度与观察角度无关，因为一个点处的不透明度，它自身就已经确定了，人类改变观察它的视角，也是不能改变它的不透明度。
* 空间中某点的颜色，是受位置与观察角度的影响。
  
所以预测体积密度$\sigma$的网络部分输入仅仅是位置**x**，预测颜色**c**的网络的输入是视角方向**d**。具体实现为：

* MLP网络F<sub>$\theta$</sub>首先用 8 层的全连接层（使用 ReLU 激活函数，每层有 256 个通道），处理 3D 坐标**x**，得到体积密度$\sigma$和一个256维的特征向量。
* 将该256维的特征向量与视角方向**d**concact起来，喂给另一个全连接层（使用 ReLU 激活函数，每层有 128 个通道），输出方向相关的 RGB 颜色。

所谓的5D神经辐射场将场景表示为：**其所在空间中任意点的体素密度和有方向的辐射亮度**。体积密度$\sigma$(x)定义为光线停留在位置x处无穷小粒子的可导概率（或者也可以理解为光线穿过此点后终止的概率）。
那么对于某个视角O发出的方向为d的光线，其在t时刻的到达点为：`r(t)=o+td`。
沿着这个方向在范围(t<sub>n</sub>,t<sub>f</sub>)对颜色积分，获得最终的颜色值*C*(r)为：
![](/img\Nerf\C(r).png)
其中,*T(t)*表示从t<sub>n</sub>到t累计的透明度，换句话说就是光线从t<sub>n</sub>到t没有碰到任何粒子的概率。因此视图的渲染就是对于*C*(r)的积分，是就是虚拟相机穿过每个像素的相机光线，所得到的颜色。不过在实际中，连续的积分是不现实的，因此使用求积法进行积分的数值求解

### 优化
* Positional Encoding (位置编码)：通过这一策略，能够使得 MLP 更好地表示高频信息，从而得到丰富的细节；
* Hierarchical Sampling Procedure (金字塔采样方案)：通过这一策略，能够使得训练过程更高效地采样高频信息。
#### Positional Encoding (位置编码)
将F<sub>$\theta$</sub>修改成两个函数的组合：F<sub>$\theta$</sub>=F<sub>$\theta$</sub><sup>'</sup>o$\gamma$,通过这样方式提升网络的性能。
$\gamma$表示从低维到高维的编码函数。
F<sub>$\theta$</sub><sup>'</sup>是正常的MLP函数。

#### Hierarchical Sampling Procedure (金字塔采样方案)
如果采样点过多计算效率太低，采样点太少又不能很好地近似整个三维场景。一个自然的想法就是对于颜色贡献大的点附件进行密集采样，贡献少的就稀疏采样，基于这一想法，NeRF 很自然地提出由粗到细的分层采样方案（Coarse to Fine）。


---

## object-Nerf
目的：
传统的Nerf渲染场景的时候，将整个场景作为整体，统一的渲染，这样的结果缺少了可编辑性，所以有较大的提升空间。在object-Nerf中解决了这个问题，不仅仅对整个场景进行渲染，也可以对场景中的物体进行编辑(旋转、平移、复制等)，实现可编辑的Nerf。

![](/img\Nerf\Object-Nerf示意图.png)
<center>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">object-Nerf示意图</div></center>

![](/img\Nerf\Object-nerf_网络结构.png)
<center>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">object-Nerf网络结构</div></center>

上图为object-Nerf的网络结构。对于object-Nerf而言，它的输入是空间中某一点x它的混合空间嵌入特征向量，
混合空间嵌入特征向量由体素特征与位置坐标特征组成，体素特征指的是将三维空间体素化，对于x所待的体素块，周围由其它8个体素块围着它，那就由其它8个体素块组成的特征，描述x所在的体素块。
从上图可以看到，object-Nerf网络由scene branch和object-branch两个分支组成。
### scene branch
scene branch就和传统的Nerf结构一样，学习整个三维空间的表达。它学到的东西，有两个作用：1.作为后续object-Nerf的背景 2.为后面的object-branch中，判断物体是否被遮挡。

### object branch
object branch的功能就是，对每个单个物体进行渲染。在object branch中，它是以mask作为约束，只学习mask中物体的颜色属性、不透明度等等，对每个单独物体进行Nerf化。同时，场景中一般有多个物体，那么获得相应的n个物体的激活码。在渲染过程中，我们对于在mask中的每一条射线，它对应一个物体，我们为该射线赋予对应的激活码，让激活码和我们物体关联起来，这样再后面的渲染时，输入对应的激活码，生成对应的物体。

### scene-guided occlusion identification
在现实中，物体之间有遮挡是很正常的情况，那在网络中，如果我们将两个物体的遮挡交界处学到，那势必会对这个物体的Nerf化有影响。作者人为的将相机视角，往前推了一部分，这样就避开了物体的遮挡交界处，虽然学到的部分表少了，但是物体不会收到遮挡交界处的影响，再通过其他视角对于遮挡部分进行补充。
总之：只学习未被遮挡的部分，不学习遮挡区域。

---
## instant-ngp
instant-ngp是今年NVIDIA在SIGGRAPH 2022中的项目，由于其"5s训练一个Nerf"的传奇速度，受到研究人员的关注。下面对其做简单介绍，也作为自己学习的记录。

#### 背景
传统基于全连接的神经网络已经能解决很多问题，比如MLP结构(*PointNet、Nerf等*)，但是这种全连接的神经网络在训练和评估中代价很大。同时在基于深度学习的图形学任务中，每个工作都针对自己特定的task，设计不同的网络结构，这样的缺点是这些方法只限制在特定的任务上，同时这些工作在优化整个网络的过程中，需要对整个网络进行优化，这加大了网络的花费。

### 方案
作者提出一种基于多分辨率的哈希编码方案，这个方案与task无关，是一种通用的方案(改方案能用在多种不同任务中)，改方案只由参数*T*和期望的分辨率*N*<sub>mak</sub>配置，实验结果表明该方案在多项任务上达到了不错的效果。该方案与任务无关的**自适应性**和**效率**的关键是哈希表的多分辨率层次结构。
### Adaptivity
作者将网格级联的映射到相应的固定大小的特征向量数组。在粗分辨率下，从网格点到数组条目有一个1：1的映射。在精细分辨率下，数组被视为一个哈希表，并使用空间哈希函数进行索引，其中多个网格点为每个数组条目起别名。
这种哈希碰撞将导致训练梯度达到平均水平，这意味着最大的梯度(与损失函数最相关的梯度)将占主导地位，因此哈希表将自动对最重要且细节最丰富的部分做优先考虑。同时与之前的工作不同，在instant-ngp的训练期间不需要对数据结构进行系统性更新。
### Efficiency
哈希表的查找是O(1),同时哈希表可以并行的查找。

### MultiResolution Hash Encoding
![](/img\instant-ngp\Hash_encoding_parameters.png)
<center>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"> Hash_encoding_parameters table</div></center>


![](/img\instant-ngp\multiresolution_hash_encoding_示意图.png)
<center>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"> Illustration of the multiresolution hash encoding in 2D</div></center>

一个全连接网络 *m*(y;$\phi$)，对输入的y进行编码*y*=enc(x;$\theta$)，因此在instant-ngp中，既有可训练的权值参数$\phi$，也有可训练的编码参数$\theta$。这些参数被分布为L个层，每层中包含T个维度为F的特征向量。上面的表1为哈希编码参数，**只有哈希表的大小*T*和期望的分辨率*N*<sub>mak</sub>需要被设置(作为超参数 )**。

在这个过程中，每个层是独立的(*如上图中的红蓝两层，分别代表不同层级。红色代表分辨率较高的体素网格、蓝色代表分辨率较低的体素网格*)，同时存储网格顶点所代表的特征向量，每一层的分辨率是最糙分辨率与最佳分辨率之间的几何级数。[*N*<sub>min</sub>,*N*<sub>mak</sub>]。*N*<sub>mak</sub>代表了数据中的最好细节。

由上面的陈述我们知道参数被分为L个层，按照表1给出的超参数可知L被设置为16，即体素的分辨率有16个等级，那么分辨率层级从低到高的变化公式为：

![](/img/instant-ngp/层级分辨率变化公式.png)

其中b被认为是成长参数，作者将其设置为1.38到2之间。对于L层中的某个层l，输入的坐标点对应的分辨率应该为$\lfloor $x<sub>𝑙</sub>$ \rfloor$:=$\lfloor x* $ *N*<sub>𝑙</sub> $ \rfloor$,$\rceil $x<sub>𝑙</sub>$\rceil$:=$\lceil x* $ *N*<sub>𝑙</sub> $ \rceil$。

对于较粗糙的网格，不需要T个参数，它的参数量应该为(*N*<sub>𝑙</sub>)<sup>d</sup>&le;T，这样以来映射就可以保证一对一的关系，对于精细网格，使用空间哈希函数*h*:*Z*<sup>d</sup>&rarr;*Z*<sub>T</sub>将网格索引到数组。本文中空间哈希函数的定义为

![](/img/instant-ngp/空间哈希函数.png)

运算符$\bigoplus$表示异或运算，$\pi$<sub>i</sub>表示独一无二大小的大素数，改计算在每一维产生线性同余排列，以解除维度对哈希值的影响。最后根据x在体素中的相对位置，实现体素内角点特征向量的线性插值，插值的权重为*W*<sub>𝑙</sub>:=*x*<sub>𝑙</sub>-$\lfloor $x<sub>𝑙</sub>$ \rfloor$(我猜这里应该代表可视化图中，线性插值的部分，但还是有些不理解)。

根据以上对哈希编码的陈述，可以理解这个过程在L层中可以独立的执行，不会相互干扰。这样的话每一层的插值向量以及辅助输入(位置+视角方向)被串联以产生y，y是由输入的enc(x;𝜃)到MLP m(y;Φ)。 

### 神经网络训练参数
为了优化GPU的缓存，作者逐级的处理对应的哈希表，当处理一批输入时，首先计算多分辨率哈希编码的第一级，接着是第二级，依次类推。所以在整个过程中，只有少量的哈希表需要驻留在GPU缓存中，节省了GPU的内存。